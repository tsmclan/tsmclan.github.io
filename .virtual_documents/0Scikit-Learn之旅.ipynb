





get_ipython().run_line_magic("matplotlib", " inline")
import time
import numpy as np
import matplotlib.pyplot as plt

from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import r2_score
from sklearn.metrics import classification_report
# cross_validation used to exist in package sklearn but was deprecated at sklearn 0.21.1
import sklearn
print("Scikit-Learn Ver: {0}".format(sklearn.__version__))
print(sklearn.__doc__)
# from sklearn import cross_validation as cv              # deprecated
from sklearn.model_selection import cross_val_score as cv # now in use
from sklearn.model_selection import train_test_split

# Load the example datasets
from sklearn.datasets import load_boston
from sklearn.datasets import load_iris
from sklearn.datasets import load_diabetes
from sklearn.datasets import load_digits
from sklearn.datasets import load_linnerud

# Boston house prices dataset (reals, regression)
boston = load_boston()
print("{0:>9} {ds[0]:>4} samples {ds[1]:>2} features".format("Boston:", ds=boston.data.shape))

# Iris flower dataset (reals, multi-label classification)
iris   = load_iris()
print("{0:>9} {ds[0]:>4} samples {ds[1]:>2} features".format("Iris:", ds=iris.data.shape))

# Diabetes dataset (reals, regression)
diabetes = load_diabetes()
print("{0:>9} {ds[0]:>4} samples {ds[1]:>2} features".format("Diabetes:", ds=diabetes.data.shape))

# Hand-written digit dataset (multi-label classification)
digits = load_digits()
print("{0:>9} {ds[0]:>4} samples {ds[1]:>2} features".format("Digits:", ds=digits.data.shape))

# Linnerud psychological and exercise dataset (multivariate regression)
linnerud = load_linnerud()
print("{0:>9} {ds[0]:>4} samples {ds[1]:>2} features".format("Linnerud:", ds=linnerud.data.shape))





import pandas as pd
from pandas.plotting import scatter_matrix
X = iris.data
y = iris.target
df = pd.DataFrame(X, columns = iris.feature_names)
fig = scatter_matrix(df, alpha=0.2, figsize=(16, 10), diagonal='hist', marker='o', c=y)


X = diabetes.data
y = diabetes.target
df = pd.DataFrame(X, columns = diabetes.feature_names)
fig = scatter_matrix(df, alpha=0.2, figsize=(16, 10), diagonal='kde', c=y)





from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(diabetes.data, diabetes.target, random_state=0)

# Fit a given model to diabetes dataset and evaluate it
def train_evaluate(model, prn=True):
    model.fit(X_train, y_train)
    expected  = y_test
    predicted = model.predict(X_test)
    # Evaluate fit of the model
    _mse = mse(expected, predicted)
    _r2 = r2_score(expected, predicted)
    if prn:
        print("Mean Squared Error: {0:0.3f}".format(_mse))
        print("Coefficient of Determination: {0:0.3f}".format(_r2))
    else:
        return _mse, _r2





from sklearn.linear_model import LinearRegression

# Fit regression to diabetes dataset and evaluate it
model = LinearRegression()
train_evaluate(model)





from sklearn.linear_model import Perceptron

# Fit perceptron to diabetes dataset and evaluate it
model = Perceptron()
train_evaluate(model)





from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split

# Fit regression to diabetes dataset
model = KNeighborsRegressor()
train_evaluate(model)





from sklearn.neighbors import KNeighborsRegressor

# Fit regression to diabetes dataset and evaluate it
model = RandomForestRegressor(n_estimators=60)
train_evaluate(model)





from sklearn.ensemble import RandomForestRegressor

# Fit the model to diabetes dataset and evaluate it
model = RandomForestRegressor(n_estimators=60)
train_evaluate(model)





from sklearn.ensemble import AdaBoostRegressor

model = AdaBoostRegressor()
train_evaluate(model)





from sklearn.svm import SVR

model = SVR(gamma='auto')
train_evaluate(model)





from sklearn.linear_model import Ridge

model = Ridge(alpha=0.1)
train_evaluate(model)





from sklearn.linear_model import Lasso

model = Lasso(alpha=0.1)
train_evaluate(model)





from sklearn.model_selection import train_test_split

# Fit a given model to iris dataset and evaluate it
X_train,X_test,y_train,y_test = train_test_split(iris.data, iris.target, test_size=0.2)
def train_evaluate(model, prn=True):
    model.fit(X_train, y_train)
    expected  = y_test
    predicted = model.predict(X_test)
    if prn:
        print(classification_report(expected, predicted))








from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='lbfgs', multi_class='auto')
train_evaluate(model)








#from sklearn.lda import LDA  # deprecated at sklearn 0.16
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

model = LDA()
train_evaluate(model)





from sklearn.naive_bayes import GaussianNB

model = GaussianNB()
train_evaluate(model)





from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier()
train_evaluate(model)





from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier()
train_evaluate(model)





from sklearn.svm import SVC

kernels = ['linear', 'poly', 'rbf']
for kernel in kernels:
    print('kernel: {0:<6} ---------------------'.format(kernel))
    if kernel != 'poly':
        model      = SVC(kernel=kernel, gamma='auto')
    else:
        model      = SVC(kernel=kernel, gamma='auto', degree=3)
        
    train_evaluate(model)





from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=60)
train_evaluate(model)





from sklearn import svm
X_train,X_test,y_train,y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)
model = svm.SVC(kernel='linear', C=1, random_state=42)
scores = cv(model, X_test, y_test, cv=5, scoring='f1_macro')
print("Accuracy: {0:0.2f} \t\tStandard deviation: {1:0.2f}".format(scores.mean(), scores.std()))
scores.round(3)





from sklearn.model_selection import ShuffleSplit
from sklearn import svm

n_samples = X_test.shape[0]
_cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
model = svm.SVC(kernel='linear', C=1, random_state=42)
scores = cv(model, X_test, y_test, cv=_cv, scoring='f1_macro')
print("Accuracy: {0:0.2f} \t\tStandard deviation: {1:0.2f}".format(scores.mean(), scores.std()))
scores.round(3)





from sklearn import preprocessing
from sklearn import svm

scaler = preprocessing.StandardScaler().fit(X_train)
X_train_transformed = scaler.transform(X_train)
clf = svm.SVC(C=1).fit(X_train_transformed, y_train)
X_test_transformed = scaler.transform(X_test)
clf.score(X_test_transformed, y_test).round(3)





from sklearn.model_selection import ShuffleSplit
from sklearn import preprocessing
from sklearn import svm

n_samples = X_test.shape[0]
_cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(kernel='linear', C=1, random_state=42))
score = cv(clf, X_test, y_test, cv=_cv, scoring='f1_macro')
print("Accuracy: {0:0.2f} \t\tStandard deviation: {1:0.2f}".format(scores.mean(), scores.std()))
scores.round(3)





get_ipython().run_line_magic("matplotlib", " inline")
import time
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_circles
from sklearn.datasets import make_moons
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

N = 1000 # Number of samples in each cluster

# Some colors for later
colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])
colors = np.hstack([colors] * 20)

circles = make_circles(n_samples=N, factor=.5, noise=.05)
moons   = make_moons(n_samples=N, noise=.08)
blobs   = make_blobs(n_samples=N, random_state=9)
noise   = np.random.rand(N, 2), None

# Let's see what the data looks like!
fig, axe = plt.subplots(figsize=(18, 4))
for idx, dataset in enumerate((circles, moons, blobs, noise)):
    X, y = dataset
    X = StandardScaler().fit_transform(X)
    
    plt.subplot(1,4,idx+1)
    plt.scatter(X[:,0], X[:,1], marker='.')

    plt.xticks(())
    plt.yticks(())
    plt.ylabel('$x_1$')
    plt.xlabel('$x_0$')

plt.show()

# Train the model and plot the predictions
def train_plot(model):
    fig, axe = plt.subplots(figsize=(18, 4))
    for idx, dataset in enumerate((circles, moons, blobs, noise)):
        X, y = dataset
        X = StandardScaler().fit_transform(X)

        # Fit the model and make predictions
        model.fit(X)
        predictions = model.predict(X)

        # Find centers
        centers = model.cluster_centers_
        center_colors = colors[:len(centers)]
        plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)

        plt.subplot(1,4,idx+1)
        plt.scatter(X[:, 0], X[:, 1], color=colors[predictions].tolist(), s=10)

        plt.xticks(())
        plt.yticks(())
        plt.ylabel('$x_1$')
        plt.xlabel('$x_0$')

    plt.show()





from sklearn.cluster import MiniBatchKMeans

# Fit the model with our algorithm
model = MiniBatchKMeans(n_clusters=4)
train_plot(model)





from sklearn.cluster import AffinityPropagation

# Fit the model with our algorithm
model = AffinityPropagation(damping=.9, preference=-200)
train_plot(model)





import os.path as path
import os, requests

# Download dataset from the web and store it to a local path
def download_dataset(fromURL, toPath):
    os.makedirs(path.dirname(toPath), exist_ok=True)
    r = requests.get(fromURL)
    with open(toPath,'wb') as f:
        f.write(r.content)
    r.close()

# download remotely from
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.csv"
# where to store the dataset locally
HOUSING_PATH = path.join("data", path.basename(HOUSING_URL))
download_dataset(HOUSING_URL, HOUSING_PATH)


get_ipython().run_line_magic("matplotlib", " inline")
import time
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# show what the data is like
housing = pd.read_csv("data/housing.csv")
housing.hist(bins=50, figsize=(20,15))
# new a column named as income_cat
# np.ceil向上取整
housing["income_cat"] = np.ceil(housing["median_income"] / 1.5)
# 除小于5的值以外都改为5
housing["income_cat"].where(housing["income_cat"] < 5, 5.0, inplace=True)
print(housing.head(n=3))





from sklearn.impute import SimpleImputer as Imputer

#删除房价列后复制给新的变量。该函数默认删除行，加axis=1表示删除列
housing = strat_train_set.drop("median_house_value", axis=1)
housing_labels = strat_train_set["median_house_value"].copy()
#删除非数据类型的属性
housing_num = housing.drop("ocean_proximity", axis=1)

#建立一个填补缺失的工具imputer，策略是使用中位数
imputer = Imputer(strategy="median")
#将imputer拟合到数据集
imputer.fit(housing_num)
#将缺失值替换为中位数
X = imputer.transform(housing_num)
#将X(这是numpy数组)转换为Pandas的Dataframe数据集类型
housing_tr = pd.DataFrame(X, columns=housing_num.columns)
print(">>>>>>>>>>>>>>>> Housing".format(housing.info(verbose=False)))
print(">>>>>>>>>>>>>>>> Housing imputed".format(housing_tr.info(verbose=False)))





from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]): 
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]

# remove the temporary column income_cat
for set in (strat_train_set, strat_test_set):
    set.drop(["income_cat"], axis=1, inplace=True)





# have a copy of the stratified 
housing = strat_train_set.copy()
# create a correlation matrix using
corr_matrix = housing.corr()
print(corr_matrix["median_house_value"].sort_values(ascending=False))
# get a visual representation of the correlation
housing.plot(kind="scatter", x="median_income",y="median_house_value",alpha=0.1)
#s表示半径，指定为人口数/100
#c表示颜色，指定根据房价变化
#cmap是颜色集，可以自动根据数值大小选定颜色
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, 
    s=housing["population"]/100, label="population",
    c="median_house_value",  colorbar=True)






