<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>/*GitHub*/
.highlight {background-color:#fff;color:#333333;}
.highlight .hll {background-color:#ffffcc;}
.highlight .c{color:#999988;font-style:italic}
.highlight .err{color:#a61717;background-color:#e3d2d2}
.highlight .k{font-weight:bold}
.highlight .o{font-weight:bold}
.highlight .cm{color:#999988;font-style:italic}
.highlight .cp{color:#999999;font-weight:bold}
.highlight .c1{color:#999988;font-style:italic}
.highlight .cs{color:#999999;font-weight:bold;font-style:italic}
.highlight .gd{color:#000000;background-color:#ffdddd}
.highlight .ge{font-style:italic}
.highlight .gr{color:#aa0000}
.highlight .gh{color:#999999}
.highlight .gi{color:#000000;background-color:#ddffdd}
.highlight .go{color:#888888}
.highlight .gp{color:#555555}
.highlight .gs{font-weight:bold}
.highlight .gu{color:#800080;font-weight:bold}
.highlight .gt{color:#aa0000}
.highlight .kc{font-weight:bold}
.highlight .kd{font-weight:bold}
.highlight .kn{font-weight:bold}
.highlight .kp{font-weight:bold}
.highlight .kr{font-weight:bold}
.highlight .kt{color:#445588;font-weight:bold}
.highlight .m{color:#009999}
.highlight .s{color:#dd1144}
.highlight .n{color:#333333}
.highlight .na{color:teal}
.highlight .nb{color:#0086b3}
.highlight .nc{color:#445588;font-weight:bold}
.highlight .no{color:teal}
.highlight .ni{color:purple}
.highlight .ne{color:#990000;font-weight:bold}
.highlight .nf{color:#990000;font-weight:bold}
.highlight .nn{color:#555555}
.highlight .nt{color:navy}
.highlight .nv{color:teal}
.highlight .ow{font-weight:bold}
.highlight .w{color:#bbbbbb}
.highlight .mf{color:#009999}
.highlight .mh{color:#009999}
.highlight .mi{color:#009999}
.highlight .mo{color:#009999}
.highlight .sb{color:#dd1144}
.highlight .sc{color:#dd1144}
.highlight .sd{color:#dd1144}
.highlight .s2{color:#dd1144}
.highlight .se{color:#dd1144}
.highlight .sh{color:#dd1144}
.highlight .si{color:#dd1144}
.highlight .sx{color:#dd1144}
.highlight .sr{color:#009926}
.highlight .s1{color:#dd1144}
.highlight .ss{color:#990073}
.highlight .bp{color:#999999}
.highlight .vc{color:teal}
.highlight .vg{color:teal}
.highlight .vi{color:teal}
.highlight .il{color:#009999}
.highlight .gc{color:#999;background-color:#EAF2F5}
</style><title>cvpr</title></head><body><article class="markdown-body"><p><link type="image/x-icon" rel="shortcut icon" href="/Users/rchen/D4win/wk/tsmc/pap/cvpr/favicon.ico" />
<link href="/_assets/font-awesome.css" rel="stylesheet"/>
<link href="/_assets/prismjs.css" rel="stylesheet" />
<link href="/_assets/markdown.css" rel="stylesheet" />
<script type="text/javascript" src="/_assets/prism.js"></script>
<!-- NOTE: There are several steps that are taken to release the html document -->
<!-- Step 1: remove the title, head, article tags generated by MarkdownPreview -->
<!-- Step 2: uncomment the following lines for enabling MathJax script -->
<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    TeX: { equationNumbers: { autoNumber: "AMS" } },
    tex2jax: {inlineMath: [["$","$"]]}
  });
</script>
<script type="text/javascript" async src="/MathJax/MathJax.js?config=TeX-MML-AM_CHTML"></script> --></p>
<!-- Step 3: uncomment the following lines for enabling toggle-toc script -->

<!-- <script type="text/javascript" src="/_assets/jquery.min.js"></script>
<script>
$(document).ready(function(){
  $(".toc-toggle").click(function(){
    $('body').toggleClass('with-toc');
  });
});
</script> -->

<p><title>Computer Vision and Pattern Recognization (CVPR)</title>
<!-- Step 1.1: uncomment the next line when revising the head tag in Step 1 -->
<!-- <body class="with-toc"><article class="markdown-body"> --></p>
<h1 id="early-drafts-on-cvpr-2018-2020">Early drafts on CVPR (2018-2020)<a id="home"></a><a class="headerlink" href="#early-drafts-on-cvpr-2018-2020" title="Permanent link"></a></h1>
<p>
始于：2018-04-15 (周日) 05:41&nbsp;
出处：<b>tsmc 原创</b>&nbsp;
作者：<b>陈荣</b>&nbsp;
止于：2020-11-17 (周二) 08:24
</p>

<!-- Navigation section -->

<!-- http://web.archive.org/web/20130929055652/http://fontawesome.io/ v3.2.1-->

<!-- tp://localhost/faicons.html -->

<!-- <I class="icon-compass"></I> icon-compass -->

<ul id="breadcrumb">
<li><a href="#home"><span class="icon icon-home"> </span> &nbsp;</a></li>
<li><a href="#derain"><span class="icon icon-twitter"> </span> 去雨</a></li>
<li><a href="#dehaze"><span class="icon icon-comment-alt"> </span> 去雾</a></li>
<li><a href="#fn:1"><span class="icon icon-bookmark-empty"> </span> Private Ref.</a></li>
<li><a href="/rdings/mostcited.html#cvpr"><span class="icon icon-bookmark-empty"> </span> Topic Ref.</a></li>
<li><a href="#his"><span class="icon icon-rss"> </span> Work History</a></li>
</ul>

<p><br></p>
<p><a href="http://cvlab.cse.msu.edu/tag/face-recognition.html">Computer Vision Lab</a> at Michigan State University</p>
<p><a href="/rd/nc/TCDCN.pdf">TCDCN-Tasks Constrained Deep Convolutional Network</a> 第一篇多任务CNN - Facial Landmark Detection by Deep Multi-task Learning <sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>
<a href="/rd/nc/mtCNN.pdf">mtCNN - Multi-Task Convolutional Neural Network</a> 第二篇多任务CNN - Multi-Task Convolutional Neural Network for Pose-Invariant Face Recognition <sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>
<a href="/rd/nc/HyperFace.pdf">HyperFace</a> 第三篇多任务CNN - HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition <sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup></p>
<h2 id="single-image-dehazing-via-depth-guided-deep-retinex-decomposition">Single-image Dehazing via Depth-guided Deep Retinex Decomposition <a id="PRL"></a><a class="headerlink" href="#single-image-dehazing-via-depth-guided-deep-retinex-decomposition" title="Permanent link"></a></h2>
<p>In this paper, we explore the problem of single-image haze removal based on retinex model and new deep retinex decomposition architecture. And we address the limitation of the estimated atmospheric scattering model on which the state-of-the-art deep learning methods are prone to make errors. Reformulating dehazing as reverse retinex, we propose a depth-guided retinex decomposition network, which consists of the Decom-Net with two-branches for the retinex decomposition on reversed hazy image, and the Illumination-Net with depth information for de-noising the illumination image. Extending the U-Net, we develop an eﬀective boosted decoder with a fusion attention mechanism to  optimiz the illumination iteratively, leading to a refined reflectance. Through sets of experiments on a variety of synthetic and natural images, we validate the effectiveness of the proposed model in haze removal, competitively in terms of visual appearance and metrics, especially in high-light and low-light hazy regions.</p>
<h2 id="multi-task-learning-enhanced-single-image-de-raining">Multi-Task Learning Enhanced Single Image De-Raining <a id="tip"></a><a class="headerlink" href="#multi-task-learning-enhanced-single-image-de-raining" title="Permanent link"></a></h2>
<p><a href="/rdings/mostcited.html#fn:102" target="_blank">[Yasarla et al., 2020]</a> proposed a semi-supervised deep network that generates the pseudo ground truth (GT) of the unlabeled data through a non-parametric Gaussian process (GP) trained on the latent space of labeled data, and learns to derain after minimizing the error between the projections of unlabeled real-world images and the pseudo GT of synthetic images.</p>
<!-- Derainet: A Novel Deep Learning Model for Single Image Rain Removal -->

<!-- Deep Pixel Shuffle Network with Gumbel-Softmax-Guided Joint Optimization for Rain Removal from Single Image -->

<!-- Deep Pixel Shuffle Network with Gumbel-Softmax-Guided Optimization for Rain Removal from Single Image -->

<h2 id="single-image-rain-removal-via-deep-pixel-shuffle-network-withgumbel-softmax-gate">Single Image Rain Removal via Deep Pixel Shuffle Network withGumbel-Softmax Gate <a id="acmmm"></a><a class="headerlink" href="#single-image-rain-removal-via-deep-pixel-shuffle-network-withgumbel-softmax-gate" title="Permanent link"></a></h2>
<p><code>start</code>(2020-04-05 21:59) <code>end</code>(2020-05-23 10:00)</p>
<p><i class="icon-bullseye"></i> <a href="/pap/cvpr/ACM multimedia 2020/张天伦ACMMM0402.pdf">张天伦ACMMM0402.pdf</a>&nbsp;&nbsp;
<i class="icon-folder-open"> D:\wk\tsmc\pap\cvpr\ACM multimedia 2020\</i></p>
<p>In this paper, we propose a layer extraction model for single image rain removal based on a novel deep neural network in order to address limitations that existing networks face. The First is their large computational cost and downgrade risk in the progressive down- and up-sampling. In point we conduct an efficient representation learning by using pixel shuffle to scale features. Second, to curtail redundant information, we incorporates Gumbel-Softmax gate into an unsupervised learning for the meta-features that play the role of masks to let the network pay more attention on effective information. Last, without any prior knowledge, we develop a multi-task learning framework and use a parametric adaptive evolutionary algorithm to enhance the synergy among different deraining tasks. Putting them together, we can demonstrate the effectiveness of the proposed model in the rain layer separation from a single observation throughout experiments on a variety of synthetic and natural images. </p>
<p>==&gt;</p>
<pre><code>摘要部分，总逻辑建议顺着我之前写的3.1 的那个逻辑来，具体而言宏观层面强调，1. 目前普通CNN及GAN网络表征在derain这个问题的缺陷 2. rain 和 background 在特征层面进行分离处理的重要性 3. 目前loss function没有考虑rainy 重建
</code></pre>
<p>In this paper, we explore the problem of single image rain removal based on new rain image model and new deep learning architecture. And we address three limitations that the state-of-the-art deep learning methods face. The first is the background information missing inevitably in current deep networks due to the down/up-sampling operations, for which we develop an efficient representation learning by using pixel shuffle to scale features. Second, contextual background information is illusive. We instead look for saliency features of rain and of background by an unsupervised learning with unique Gumbel-Softmax gate, which is quite beneficial to rain separation. The last is that loss functions typically used by segmentation tasks do not consider the rainy image formulation process. We respond by proposing three model-specific loss functions and put them into a multi-task learning framework with a parametric adaptive evolutionary algorithm to increase their synergy. Through sets of experiments on a variety of synthetic and natural images, we validate the effectiveness of the proposed model in rain removal, competitively in terms of visual appearance and metrics.</p>
<p>In the down- and up-sampling processes, we use a pixel shuffle technique to scale the size of feature maps. By doing so, the deraining network can avoid to miss information and increase coarse interpolation. Then a Gumbel-Softmax gate is designed to learn the masks of rain and background features, and the redundant features can be filtered out by the masks.</p>
<p>Many methods have been proposed to address the restoration of rain degradation. Some focus on video deraining [2], [4], [5], [7], [16]–[19], [58]. Others focus on single image rain removal. These methods regard the rain streak removal problem as a signal separation problem [7], [24], [30], [37], [46], [48], or by relying on nonlocal mean smoothing [32]. These methods have made progress to some extent, however, they still suffer from some limitations. Because the rain streaks and background textures are overlapped intrinsically in the feature space, most methods cause non-rain regions to lose texture details and be over-smoothed.</p>
<p>The rain degradation can be complex, and previous widely used rain models (e.g., [7], [30]) neglect some important visual factors of real rain images, such as the atmospheric veils caused by rain streak accumulation, and different shapes or directions of streaks. Moreover, many existing algorithms operate in a patchwise way with a limited receptive ﬁeld (a limited spatial range). Thus, spatial contextual information in larger regions is absent, which in fact has been proven to be useful for rain removal [25].</p>
<p>To address these limitations, we make effort to develop a novel rain model that explicitly describes various rain conditions in real scenes, including rain streak accumulation and heavy rain, and then, design an effective deep learning architecture based on the novel rain model. Here, we focus on a single input image. Our ideas are as follows.</p>
<p>Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. </p>
<p>There has been a great diversity of approaches and methodology developed,rather than a single unified retrieval model that has proven to be most effective; however, the field has progressed in two different ways. On the one hand, theoretical studies of an underlying model have been developed; this direction is, for example, represented by the various kinds of logic models and probabilistic models (e.g., [14, 3, 15, 22]). On the other hand, there have been many empirical studies of models, including many variants of the vector space model (e.g., [17,18, 19]). In some cases, there have been theoretically motivated models that also perform well empirically; for example, the BM25 retrieval function, motivated by the 2-Poisson probabilistic retrieval model, has proven to be quite effective in practice [16].</p>
<p>==&gt; </p>
<p>Rain brings sharp intensity fluctuations in images and videos, often degrading the performance of outdoor vision systems. Mathematically, this problem is formulated as $\mathbf{O}=\mathbf{B}+\mathbf{R}$, where $\mathbf{O}$ is the observed rain image, $\mathbf{B}$ the desired background layer and the rain streak layer $\mathbf{R}$. As ill-posed over the decades, many approaches have been proposed to mitigate the difficulty of the background scene recovery given the rich information in video sequences and multiple relevant images to detect rain streaks. The information they exploited includes factors affecting their visual appearance [5], features such as their frequency domain analysis [2a], histogram of orientation [4a] and generalized low rank [7a], as well as the man-made prior knowledge [2]. </p>
<p>The rain removal problem with single image is more ill-posed than that with videos or multiple images due to fewer temporal information to exploit. To alleviate the problem, some statistical methods are employed to learn the Gaussian mixture models from available observations to characterize the general peculiarity of rains [10]. Other techniques separate the rain streaks from the high frequency layer by sparse coding (e.g. [19, 13]). All of these methods are criticized for their over-smoothness of the background.  </p>
<p>Yet despite various solutions have been proposed, none of which are totally satisfactory when facing real rainy images. The rain layer separation can be even complex in practice if exists the atmospheric veils caused by rain streak accumulation, and different shapes or directions of streaks []. </p>
<pre><code>[2a] P. C. Barnum, S. Narasimhan, and T. Kanade. Analysis of rain and snow in frequency space. Int’l Journal of Computer Vision, 86(2-3):256–274, 2010.
[4a] J. Bossu, N. Hautiere, and J.-P. Tarel. Rain or snow detection in image sequences through use of a histogram of orientation of streaks. International journal of computer vision, 93(3):348–367, 2011.
[7a] Y.-L. Chen and C.-T. Hsu. A generalized low-rank appearance model for spatio-temporally correlated rain streaks. In Proceedings of the IEEE International Conference on Computer Vision, pages 1968–1975, 2013.
[37] Y. Luo, Y. Xu, and H. Ji. Removing rain from a single image via discriminative sparse coding. In Proc. IEEE Int’l Conf. Computer Vision, pages 3397–3405, 2015.
</code></pre>
<p>To address these limitations, we make effort to develop a novel deep learning model that consists of the representation learning structure and the layer extraction branches, and then, design an effective joint optimization to tune the network based on the screen blend model. We make the following contributions:</p>
<p>更多修改意见：</p>
<pre><code>Figure 1. 突出了自己的工作却忽略了别人的，建议替换为一张效果对比图。

p6. which are kindly provided by existing literatures. These ... ==&gt; which are public domain data kindly provided by existing literatures. As indicated by the regions in red boxes, these ...
p6. Figures 6, 7 and 8 show the differences of existing deraining techniques ...
p6. ==&gt; Figure 6: First Example of our method on heavey rain
p6. ==&gt; Figure 7: Second Example of our method on heavey rain
p6. ==&gt; Figure 8: Example of our method on rainy image with different directions of streaks
p6. remove almost all rain streaks from varied directions 
p6. that explicitly describes various rain conditions in real scenes, including rain streak accumulation and heavy rain

[14] Published as a conference paper at ICLR 2017
[5] 通过相机参数控制图像清晰度，更像是online版，不是post-processing这类工作
</code></pre>
<p>Deraining work based on deep learning methods can get visually compelling results in various rainy circumstances, however there still exist some open problems as discussed below.</p>
<p>==&gt;</p>
<p>Despite that deep learning methods outstand their good capacity to keep texture details, there are still various cases in reality such as heavy rain, the accumulation and the physical variation of rain streaks that can degrade their performance. Few attempts have been made to discuss some problems they face.</p>
<p>Compared with the rain mask in JORDER-E, the feature mask of our method is different in three aspects. First, our mask is a discriminant meta-feature of the shared feature output by FLN, and is learned in unsupervised manner. Second, our mask is adopted to develop the feature attention, which can cut down the useless information for the following deraining task. Last, the Gumbel-Softmax estimator makes the mask towards binary-value output, thus the mask of rain features can effectively reflect the sparsity of rain. Moreover different from the dilated convolution and pooling operator deployed in other methods, the pixel shuffle in our model can not only keep enough receptive field for spatial contextual information, but preserve the details of background. And the dynamic multi-task learning can further improve the robustness of our model with respect to different rainy scenes. Benefiting from these advantages, we can conclude that the proposed model can get competitive results in terms of visual appearance and metrics.</p>
<p>==&gt;</p>
<p>Among the approaches and methodology developed for the single image rain removal, JORDER-E is a single unified deraining model that has proven to be more effective when rain streaks are dense and significant. Compared with the rain mask in JORDER-E, the feature mask of our method is different in three aspects. First, our mask is a discriminant meta-feature of the shared feature output by FLN, and is learned in unsupervised manner. Second, our mask is adopted to develop the feature attention, which can cut down the useless information for the following deraining task. Last, the Gumbel-Softmax estimator makes the mask towards binary-value output, thus the mask of rain features can effectively reflect the sparsity of rain. Moreover, the pixel shuffle in our model, unlike the dilated convolution and pooling operator utilized by other methods, can not only keep enough receptive field for spatial contextual information, but preserve the details of background. The difference indicate that something new within the processing of rainy images. The improvements is stable because the dynamic multi-task learning can help the robustness of the present model facing with different rainy scenes. Based on these evidences and empirical results, we can conclude that the proposed model can get competitive results in terms of visual appearance and metrics.</p>
<p>We conducted several experiments on both synthetic and real-world rain images to evaluate the performance of our proposed approaches and its components. Below, we first discuss our experiment setup. Next, we present our experimental results of synthetic images, of natural images, and of ablation study.</p>
<h4 id="typos-and-errors">typos and errors<a class="headerlink" href="#typos-and-errors" title="Permanent link"></a></h4>
<pre><code>p1 摘要 new rain image model ==&gt; new sample-free deep representation
p1 摘要 three specific ==&gt; three model-specific
p1 pixel shuffle ==&gt; pixel-shuffle，题目+关键字，建议全文替换
p2 (SBM) ==&gt; 加参考文献
p2 2.3开头，问题较大，改为：
To advance the deep learning-based rain removel from single image, our work focuses on the sample-free feature learning and guided-attention for multiple task learning, for which a ...
p3. the GAN [17] ==&gt; 全名+缩写
p4. In many researches ... ==&gt; Many researches, e.g. [3], have proven that binary-valued gate outperformes sigmoid funtion in the  rain-background separation proplem.
p4. 公式（4）下数3行，Figure3 ==&gt; 缺空格
p4. 公式（4）下数4行，proposed ==&gt; propose
p4. 公式（4）下数5行，as analyzed ==&gt; 删
p5. 3.4开头 in ==&gt; In
p5. Eq.5 and Eq.7 ==&gt; 缺空格
p5. a non-trivial issue in which ==&gt; a non-trivial task in that
p5. Algorithm 1和As shown in Alg.1，不一致。
As shown in Alg.1, the proposed method first initialize ==&gt; The algorithm first initializes
p5. manner as follows ==&gt; manner, formalized as follows.
p6. and constants ... to limit ... ==&gt; and constant coefficients are obtained by empirically curve-fitting the data in order to limit ...
p6. on real-world rain images ==&gt;
on real-world rain images with diversified rain streaks in terms of shapes, sizes, directions and velocities.
p7. the white edges of windows ... ==&gt;
some shape outlines, e.g. the windows of front windshield, are somewhat blurred if derained by the advanced JORDER-E or LPNet, while they are clearly discernible when derained by our method. Especially, our method makes the fog thin and less so that more vehicles are visible.
</code></pre>
<p>As shown in Figure 4, our method performs much better than other methods in terms of the preserving saliency features, e.g., we can see more clearly the outlines of letters and bridge by comparing the zoomed-in regions.</p>
<h2 id="attentive-generative-adversarial-networkfor-removing-thin-cloud-from-a-single-remote-sensing-image">Attentive Generative Adversarial Networkfor Removing Thin Cloud from A Single Remote Sensing Image <a id="ietipr"></a><a class="headerlink" href="#attentive-generative-adversarial-networkfor-removing-thin-cloud-from-a-single-remote-sensing-image" title="Permanent link"></a></h2>
<p><strong>Journal:</strong> ipr | <strong>Submitted by:</strong> Professor Rong Chen | <strong>Submitted date:</strong> 25-03-2020 06:35</p>
<p>With the increasing endeavor in the ocean investigation, underwater image processing has emerged as a relevant research in a wide range of domains, such as underwater teleoperation [1], deep ocean exploration [2], etc. However, the underwater image suffers from blurring effect, contrast degradation and grayed out color because of the absorption and scattering effects in water. This can thereby endanger computer vision applications, especially safety-critical ones.</p>
<p>Researchers have investigated ways of improving the visual appearance of the image, or providing a “better” transform representation for future automated image processing. Some existing techniques utilize multiple images (e.g., []) or depth information (e.g., []) to improve image quality. Still other techniques use either histogram equalization-based [07,09] or tone mapping-based [] technique of modifying images to make visually acceptable. These techniques have demonstrated their effectiveness in improving medical images, satellite images, aerial images and even real life photographs that suffer from poor contrast and noise. As for the enhancement of underwater images, several model-based approaches were developped based on transfer function [4], dictionary learning [5], depth estimation [6] and wavelength compensation [7]). But due to the turbid nature of water, the multiple scattering is inevitable [9]. So available techniques generally lack in providing adequate robustness and imperceptibility requirements. </p>
<p>[07] S. S. Agaian, S. Blair and K. A. Panetta, “Transform coefficient histogram-based image enhancement algorithms using contrast entropy”, IEEE Trans. Image Processing, vol. 16, no. 3, pp. 741-758, 2007.</p>
<p>[09] YunBo Rao and Leiting Chen, “An efficient contourlet transform-based algorithm for video enhancement”, Journal of Information Hiding and Multimedia Signal Processing, vol. 2, no. 3, pp. 282-293, 2011.</p>
<p>[03] H. Wen, Y. Tian, T. Huang, and W. Gao, “Single underwater image enhancement with a new optical model,” in Proc. IEEE Int. Symp. Circuits &amp; Syst. (ISCAS), May 2013, pp. 753-756.</p>
<p>Using the image formation model, He et al. [28] proposed the dark channel prior (DCP) to remove fog/haze in natural terrestrial images via estimation of the ambient light and transmission map (TM). This motivated many underwater image restoration approaches [03]–[]. However, estimating background light and TM for underwater images based on the DCP frequently fails since red light is more attenuated than other wavelengths underwater. Recently, deep learning has drawn much attention and achieved plenty of successes in TM estimation for dehazing. But training using synthetic hazy images with bright ambient light may not suit underwater images. Fabbri et al. [20] introduced the generative adversarial network into the enhancement of underwater images. Yet strong prformance on image enhazement problems, but deep neural networks are very sensitive to the setting of their hyperparameters. </p>
<p>This paper &hellip; </p>
<h2 id="gaussian-prior-based-adaptive-synthetic-sampling-with-non-linear-sample-space-for-imbalanced-learning">Gaussian prior based Adaptive Synthetic Sampling with Non-linear Sample Space for Imbalanced Learning <a id="icdm"></a><a class="headerlink" href="#gaussian-prior-based-adaptive-synthetic-sampling-with-non-linear-sample-space-for-imbalanced-learning" title="Permanent link"></a></h2>
<p><code>end</code>(2019-05-31 08:02)</p>
<div class="pageBottom"><span class="bNum">1</span><span class="bTit">Abstraction</span><a id="icdmabs"></a><span class="tBot"><a href="#icdm" target="_self" > Top</a> </span></div>

<p>Abstract—In the presence of skewed category distribution, most learning systems have difficulties to learn the concept related to the minority class, which neccesariate . It thus is crucial to advance the imbalanced learning method. In this work, an actionable mechanic is proposed to augment the size of fewshot samples for the compensation of skewed distribution. The new synthetic samples generated by this proposed method have more reasonable spatial distribution in minority class feature space due to the alternative of linear sampling space widely deployed in existing over-sampling techniques. In addition, this proposed method minimizes the sampling uncertain and risk by integrating a prior knowledge about minority class instances. Moreover, a multi-objective optimization combined with error bound model develops this proposed method into an adaptive imbalanced learning. Comprehensive experiments have been conducted on several imbalanced issues, and the experimental results demonstrate that this method can improve the performance of different classiﬁcation algorithms.</p>
<p>Index Terms—Imbalanced learning, Error bound model, Adaptive method, Classiﬁcation algorithm, Gaussian Mixture Model</p>
<p>the learning system may have difficulties to learn the concept related to the minority class.</p>
<h2 id="deep-joint-neural-model-for-single-image-haze-removal-and-color-correction">Deep Joint Neural Model for Single Image Haze Removal and Color Correction <a id="is"></a><a class="headerlink" href="#deep-joint-neural-model-for-single-image-haze-removal-and-color-correction" title="Permanent link"></a></h2>
<p><a href="/pap/cvpr/ztl/IS-rainy_mood0530.pdf">cached</a> <code>end</code>(2019-05-31 08:02)</p>
<div class="pageBottom"><span class="bNum">1</span><span class="bTit">Abstraction</span><a id="isabs"></a><span class="tBot"><a href="#is" target="_self" > Top</a> </span></div>

<p>The poor quality haze-degraded images frequently and severely affect performance of today&rsquo;s computer vision applications. Despite that deep neural network is one appealing solution to remove haze from single image, dehazing is still unsettled because existing DehazeNet and MSCNN assume equal scattering coeﬀcient and have the problem of color distortion. In this paper, a three-stage joint learning network for single image haze removal and color correction is proposed. The first stage conducts the feature learning by using hierarchy convolutional layers with nested structure. In the second stage, cascading haze-relevant tasks are sequentially performed via a physics-driven sub-network, especially to break down the assumption of homogenous atmosphere, a branch of the sub-network estimates the scattering factor in form of two-dimensional tensor. Finally color is corrected via a chrominance constancy layer with perceptual loss for pixel-level alignment. We train the proposed network through a random initialization and an asynchronous grouped learning to set up parameters for sub-network, and enventually settle on model parameters by a joint optimization with a cyclic restoration. The eﬀectiveness of the proposed de-hazing model has been veriﬁed by extensive experiments, and most results of our method are impressive.</p>
<p><code>end</code> (2019-06-06 23:29)</p>
<p>Performance analysis shows that the proposed approach outperforms the existing state-of-the-art methods for single image dehazing.</p>
<p>We trained most of the models with about 215000 gradient steps and eventually settled on a discrete learning rate schedule with two 10-fold decreases (following Krizhevsky et al.), after about 180000 and 205000 gradient steps respectively.</p>
<p><a href="/rdings/cvpr/tanaka18JointOptimizationFramework4LearningWithNoisyLabels_CVPR.pdf">Joint Optimization Framework for Learning with Noisy Labels</a></p>
<p>P1.</p>
<p>Under hazy situation, the irradiance captured by camera from scene point tends to be attenuated due to the atmospheric absorption and scattering eﬀect from the turbid medium, such as suspended particles and water droplets. As a result, the visibility of scene content is deteriorated by haze. When suﬀering from images with visual quality decline, most outdoor vision systems, e.g. surveillance and autonomous navigation [1, 2], fail to provide favorable performance. It thus becomes crucial to develop eﬀective approaches for image haze removal.</p>
<p>==&gt;</p>
<p>Image quality is determinant of most computer vision applications, especially for surveillance [] and autonomous navigation systems []. Coming across the weather of fog and haze, camera often produce cruelly degraded images because the irradiance from scene point tends to be attenuated due to the atmospheric absorption and scattering eﬀect from the turbid medium such as suspended particles and water droplets. Image dehazing algorithms taken can minimize their adverse impact on computer vision systems on a daily basis.</p>
<p>p1. To ensure satisfactory perceived quality, the focus in de-hazing researches is to &hellip; ==&gt; The initial de-hazing technique focused on how to &hellip;</p>
<p>p1. Since there are multifarious ways that a hazy environment can obscure objects and their surroundings, one haze-corrupted image may have connection with several haze-free images. This issue could get even worse in presence of heavy haze as hiding details of objects in the scene may have little or no evidence in the given hazy image. So reconstructing the haze-obscured image is typically an ill-posed or inverse problem because there is no uniqueness or stability of solutions to de-hazing. </p>
<p>p1. Therefore, &hellip;  ==&gt; </p>
<p>Various techniques of image enhancement have been applied to the problem of removing haze from a single image, including histogram-based [1], contrast-based [2] and saturation-based [3]. In addition, methods using multiple images or depth information have also been proposed. For example, polarization based methods [4] remove the haze effect through multiple images taken with different degrees of polarization. In [2], multi-constraint based methods are applied to multiple images capturing the same scene under different weather conditions. Depth-based methods [5] require some depth information from user inputs or known 3D models. In practice, depth information or multiple hazy images are not always available.
Researchers have investigated ways of helping to automate the process of haze removal. Some existing techniques use histogram-based [], contrast-based [] or saturation-based [] image enhancement to remove haze from a single image. Other techniques perform a binary search of the memory state using multiple images (e.g., []) or depth information (e.g., []). Still other techniques are based on the XXX (e.g., [6, 7, 8, 9]). </p>
<p>Researchers have investigated ways of helping to automate the process of haze removal. Some existing techniques use histogram-based [], contrast-based [] or saturation-based [] image enhancement to remove haze from a single image. Other techniques perform a binary search of the memory state using one failing test case and one passing test
case to find likely faulty statements (e.g., [2, 15]). Still other techniques are based on the remote monitoring and statistical sampling of programs after they are deployed (e.g., [6, 7, 8, 9]). Most papers reporting these techniques also report empirical studies that evaluate the presented technique in terms of its effectiveness and effciency.</p>
<h2 id="an-uncertainty-based-incremental-learning-for-identifying-the-severity-of-bug-report">An uncertainty based incremental learning for identifying the severity of bug report <a id="bug"></a><a class="headerlink" href="#an-uncertainty-based-incremental-learning-for-identifying-the-severity-of-bug-report" title="Permanent link"></a></h2>
<p><code>start</code>(2018-11-04 03:12) <code>end</code>(2018-11-28 19:59)</p>
<div class="pageBottom"><span class="bNum">2</span><span class="bTit">Introduction</span><a id="derain2"></a><span class="tBot"><a href="#bug" target="_self" > Top</a> </span></div>

<p>In this paper we aim to offer an effective way of automatic tagging of software bug reports for severity recognition. </p>
<p>罗列意见如下：
1、引言的书写完全偏离了软件bug仓库挖掘，研究动机不吸引人；
2、为什么用UIL？它能解决软件bug仓库挖掘相关研究进展什么不足，不明不白，影响了创新性；
3、实验结果提升不显著，对比对象不全。</p>
<h2 id="removing-rain-streak-via-multi-task-convolutional-neural-network">Removing rain streak via Multi-Task Convolutional Neural Network <a id="derain"></a><a class="headerlink" href="#removing-rain-streak-via-multi-task-convolutional-neural-network" title="Permanent link"></a></h2>
<p><code>start</code>(2018-02-20 20:39) <code>end</code>(2018-03-28 19:59)</p>
<h4 id="ieee"><i class="icon-check-sign"></i> <a href="http://www.ieee.org/">IEEE</a><a class="headerlink" href="#ieee" title="Permanent link"></a></h4>
<p><strong><a href="https://easychair.org/conferences/?conf=hcomp2018">Submission</a></strong> Login with <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#114;&#99;&#104;&#101;&#110;&#64;&#100;&#108;&#109;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#114;&#99;&#104;&#101;&#110;&#64;&#100;&#108;&#109;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> / crat509601 </p>
<p><i class="icon-folder-open"> D:\wk18\tsmc\pap\nc\derain</i>
<i class="icon-download-alt"></i> <a href="/../pap/nc/derain/fanyulong_201802031545_paper-derain_CR.docx">fanyulong_201802031545_paper-derain_CR.docx</a></p>
<p><strong>Length.</strong> Papers of up to <markp>8 pages</markp> (references can extend beyond 8 pages) may be submitted. </p>
<div class="pageBottom"><span class="bNum">1</span><span class="bTit">Abstraction</span><a id="derain1"></a><span class="tBot"><a href="#hcomp" target="_self" > Top</a> </span></div>

<p>In this paper, we aim at removing visual effect of rain streak from a single image. To address this non-trivial issue well, we introduce an effective convolutional neural network (CNN) architecture which can be trained end-to-end. The proposed deep learning architecture contains dilated convolutions that are advantageous for each pixel in the output to have a large effective receptive field in the input without losing resolution. At the tail of our architecture, a separation structure is designed to separate the background layer and the rain layer. Also important is that we develop a multi-task regression model to optimize the proposal network. There is a main task to remove rain streaks and restore background. Others in the regression model are a set of auxiliary tasks used to promote the learning for main task. It is noteworthy that the perceptual loss, which measures high-level perceptual and semantic difference between images, is introduced as an auxiliary task to our model. It has been improved obviously that the capability of the model by adding the perceptual loss. We conduct several experiments on synthetic and real rain images, and achieve superior rain removal performance over the state-of-the-art approaches. The overall effect of our method is impressive, even in the decomposition of heavy rain and rain streak accumulation.</p>
<p>==&gt;</p>
<p>In this paper, we address a non-trivial issue of removing visual effect of rain streak from a single image. The proposed multi-task convolutional neural network is a specific deep learning architecture for rain removal research, with a specific focus on how auxiliary tasks can facilitate the main rain removal task. We design a CNN architecture composed of a de-rain network and a perceptual loss network, which can learn common features for these tasks and exploit the synergy among them. It is noteworthy that the perceptual loss, introduced as an auxiliary task to measures high-level perceptual and semantic difference between images, has resulted in improved network capability. We conduct several experiments on synthetic and real rain images, and achieve superior rain removal performance over the state-of-the-art approaches. The overall effect of our method is impressive, even in the decomposition of heavy rain and rain streak accumulation.</p>
<div class="pageBottom"><span class="bNum">2</span><span class="bTit">Introduction</span><a id="derain2"></a><span class="tBot"><a href="#hcomp" target="_self" > Top</a> </span></div>

<p>One challenging aspect of rain steak removal research is the constant struggle for  enriched features. Typically, research progress depends on better assumptions and priors about low-level features. But they have remained stubbornly difficult to define because rain streaks may appear anywhere with different intensity and brightness and direction, and also there are a variety of interfering factors such as atmospheric veils and strong light that can blur the shape of rain streak. As a result, some methods are insufficient to cope with these adverse factors in real rain images. This issue is even more salient for many existing methods if they works on local image patches without sufficient spatial contextual information. </p>
<p>One appealing solution is to [collect behavioral data over the Internet]. In theory, online experimentation would allow researchers to <code>...</code> However, the main obstacle to conducting <code>...</code> is finding <code>...</code></p>
<p>Recently, a number of <code>...</code> services have been developed which connect <code>...</code> with <code>...</code> Perhaps the most popular system is <code>...</code> Most importantly, there are a large number of people who use <code>...</code></p>
<p>There are a number of recent summaries about using <code>...</code> for research.</p>
<p>However, less is known about the viability of conducting <code>...</code> Such studies are unique in that they typically <code>...</code> These features present two key challenges for online data collection. First, there are technical challenges in <code>...</code> Second, experiments where memory and timing are important are likely more sensitive to incidental aspects of the testing environment that <code>...</code></p>
<p>The aim of the present paper is to validate <code>...</code> as a tool for <code>...</code> research, with a specific focus on <code>...</code></p>
<p>One appealing solution is convolutional neural networks (CNN) with hundreds of feature maps detected automatically in multiple layers. In recent years, CNNs have demonstrated a massive impact on performance of computer vision such as object detection [#refer] and semantic segmentation [#refer] because they performs non-linear transformations of the extracted features while acting as the classifier. Also a few CNN methods [#refer DDN, JORDER] have been developed for rain streak removal and empirically tested to work in a limited range of settings. Most recently a widely used machine learning technique – Multi-task learning (MTL) – has been incorporated into variants of CNNs (e.g. []), and significant progresses have achieved on face detection. Such studies are unique in that they typically exploits the synergy among the tasks that operate on the extracted features and boosts up their individual performances. In doing so, some technique on object detection treats multiple tasks as outputs (e.g. object type and location etc.), while other technique uses auxiliary tasks to train one major task instead of maximizing the performance of all tasks. However, less is known about the viability of removing rain streaks using multi-task CNNs.</p>
<p>The aim of the present paper is to validate multi-task CNN as a tool for rain removal research, with a specific focus on how auxiliary tasks can facilitate the main rain removal task. In doing so, we observe that rain removal can be treated as tasks that operate on the extracted features at two different semantic levels: perceptual level [#refer perceptual loss] and pixel level. The former is used to restore the image background based on differences between high-level image features extracted from pre-trained CNN, while the latter is used to separate the rain based on differences between pixel information. We design a CNN architecture composed of a de-rain network and a perceptual loss network, which can learn common features for these tasks and exploit the synergy among them. The de-rain network is a fully non-strided convolutional neural network without pooling for learning feature maps and seperating them to represent rain streaks and background respectively. The other is to control the perceptual loss by measuring differences between two groups of high-level features extracted from the former network output. </p>
<p>Figure 1 illustrates a real example of rain streaks removal through the proposed multi-task mechanism with perceptual information. Since the observed rainy image can be characterized as a linear superimposition of the desired background and the seperated rains, there are background layer and rain layer learnned from the de-rain network, which ends with a splitting structure of two small convolutional network to inherit a common set of features extracted from previous convolutional layers. The loss network is a VGG-16 network [#refer VGG] pre-trained on the ImageNet dataset [#refer ImageNet] and and frozen to encode the perceptual information. In training stage, one of the sibling mini-networks outputs r and the other outputs the b, both fed into the pre-trained loss network. The perceptual losses are figure out by comparing r with the ground-truth R, and also b with the ground-truth B. It is evident that VGG-16 network uses max pooling operation that can filter the higher-frequency information, namely the rain streaks as shown in Figure 1, which leads to larger loss. While minimizing the losses, the features get better at understaning rain streaks, which leads to improvements in the performances of rain removal. In testing stage, the output of main task captures the restored background. </p>
<p>multi-task regression model</p>
<p>the training process as to take a bunch of images, start with random filters, convolve, activate, calculate loss, back propagate and learn weights</p>
<h2 id="single-image-haze-removal-via-region-proposal-network">Single Image Haze Removal via Region Proposal Network <a id="dehaze"></a><a class="headerlink" href="#single-image-haze-removal-via-region-proposal-network" title="Permanent link"></a></h2>
<p><code>start</code>(2018-02-20 20:39) <code>end</code>(2018-04-15)</p>
<h4 id="ieee_1"><i class="icon-check-sign"></i> <a href="http://www.ieee.org/">IEEE</a><a class="headerlink" href="#ieee_1" title="Permanent link"></a></h4>
<p><strong><a href="https://easychair.org/conferences/?conf=hcomp2018">Submission</a></strong> Login with <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#114;&#99;&#104;&#101;&#110;&#64;&#100;&#108;&#109;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#114;&#99;&#104;&#101;&#110;&#64;&#100;&#108;&#109;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> / crat509601 </p>
<p><i class="icon-folder-open"> D:\wk19\tsmc\pap\nc\dehaze</i>
<i class="icon-download-alt"></i> <a href="/../pap/nc/dehaze/dehaze_yangxi20180208_CR.doc">dehaze_yangxi20180208_CR.doc</a></p>
<p><strong>Length.</strong> Papers of up to <markp>8 pages</markp> (references can extend beyond 8 pages) may be submitted. </p>
<div class="pageBottom"><span class="bNum">1</span><span class="bTit">Introduction</span><a id="dehaze1"></a><span class="tBot"><a href="#hcomp" target="_self" > Top</a> </span></div>

<p>「tai: 摘要主要想突出的是：第一：一个基于区域检测机制的去雾方法提出来，并且不需要先验，将整个去雾过程称为system。第二：提出一个神经网络模型，有两个网络单元。第三：试验：去雾试验，还有附加实验：水下场景增强和去雨雾」
—————————</p>
<p>This paper addresses the problem of haze removal from a single image. Haze removal can be formulated by atmospheric scattering model where the medium transmission of atmospheric is the key factor. We introduce region proposal network into the estimation of medium transmission map. And we propose a trainable end-to-end deep convolution network as the main model in this paper. The proposed network takes a whole hazy image as input and outputs the corresponding medium transmission map. Subsequently, the haze-free image can be restored by atmospheric scattering model. We conduct several comprehensive experiments on synthetic and real hazy images, and demonstrate that this simple approach removes haze better than the existing methods qualitatively and quantitatively. Besides, we further propose a light version of the proposed network. The lightweight network is suitable for low-power device and achieves impressive performance on haze removal.</p>
<p>==&gt;</p>
<p>Haze removal typically worked on a physical model to estimate how light is transmitted and lost due to absorption and scattering through the atmosphere. This paper introduces region proposal network (RPN) into the estimation of medium transmission map. We exploit the RPN&rsquo;s ability to learn adaptive regression models tailored specifically to the image to mathematically reverse the effect of haze. For the best results, we conduct several comprehensive experiments on synthetic and real hazy images, and demonstrate that this simple approach removes haze better than existing methods, both qualitatively and quantitatively. Also we propose a light version of the proposed network, which can achieve impressive performance of haze removal on low-power devices.</p>
<p>==&gt;</p>
<p>Haze removal typically works on a physical model to estimate how light is transmitted and lost due to absorption and scattering through the atmosphere. This paper proposes to estimate medium transmission map based on a region detection network, instead of various manmade priors (e.g. dark channel) as most existing work did. Without any priors or constraints, the network we propose takes full hazy images and patch-wise estimates medium transmission map, subsequently used to remove haze via an atmospheric scattering model. In doing so, we design a simple yet powerful deep convolutional neural network, which consists mainly of two kinds of network units and and can be trained in end-to-end manner. One is a module with residual structure that facilitates learning process of deep network. The other is a novel module with cascaded cross channel pool, which fuses multilevel haze-relevant features and enhances the abstraction ability of the model on a nonlinear manifold. Several comparative experiments have been conducted on synthetic and real images, through which we conclude that the proposed method achieves state-of-the-art haze removal results, qualitatively and quantitatively. Supplementary experiments further indicate that our method works better against image elements (e.g. the mist formed by heavy rain and the humidity met underwater) that often cause adverse effects. Besides, we present a light version of the proposed network, which achieves impressive performance of haze removal even on low-power devices.</p>
<div class="pageBottom"><span class="bNum">2</span><span class="bTit">Introduction</span><a id="dehaze2"></a><span class="tBot"><a href="#hcomp" target="_self" > Top</a> </span></div>

<p>「tai: 第一章写作思路：
第一段：介绍去雾的必要性，关键词：degraded。
第二段：去雾具有挑战性，不同地方的degraded程度不一样，呼应第一段，然后好多人用多张图像和附加信息去雾，最后总结他们受限。
第三段：单张图像去雾有了进展，但是基于强假设和先验。基于强假设的怎么做的，基于先验的怎么做的。最后总结他们受限。
第四段：人眼不需要这些假设先验，所以提出来基于仿生学的CNN去雾方法。特别提到DehazeNet。
第五段：我们的方法提出，我们与别的方法的不同。
第六段：我们方法的简单介绍。试验效果优于别的方法，且在其他degraded images上有好的效果」
—————————</p>
<p>Haze is a common meteorological phenomenon where dust, smoke and other dry particles obscure the clarity of the atmosphere. Outdoor images taken in bad weather usually lose contrast and fidelity caused by haze, resulting from the fact that light is absorbed and scattered by the turbid medium such as particles and water droplets in the atmosphere during the process of propagation. For these reasons, haze removal is desired in both consumer photography and computer vision applications.</p>
<p>==&gt;</p>
<p>Concomitant with a dramatic rise of computer vision applications that provide assistive technology devices and convenient services, the demand for haze removal has occurred in that haze inevitably puts devices and services in vain. To mitigate these vulnerabilities, computer vision practices recommend image enhancement. </p>
<p>==&gt;</p>
<p>{H}{aze} is a typically atmospheric phenomenon that reduces the visibility of outdoor scenes and inevitably puts visual devices in vain. The photo taken in hazy scenes is often degraded, owing to losing the color fidelity and contrast. Generally, the degraded image is visually unpleasant, and makes much texture detail information be identified with difficulty. Thus, hazy image causes unsatisfactory performance of vision applications and algorithms, which ideally assume that the input is clear image.</p>
<p>==&gt;</p>
<p>{C}{oncomitant} with a dramatic rise of computer vision applications that provide assistive technology devices and convenient services, the demand for haze removal is highly desired because haze certainly will degrade image quality and severely causes unsatisfactory performance of applications and algorithms that often assume image contrast, details, and even vivid image colors. The particles or droplets in fog and mist reduces visibility and contrast, either by the actual obscuring or light reflection, but the degradation rises nonlinearly with distance between the scene points and the lens, which makes dehazing a challenging task.
To mitigate the so-called spatial-variant degradation, image enhancement has been investigated in early research and practices, most based on saturation [1], histogram [2]-[4] or contrast [5]. However, their performance is barely acceptable because the dehazing for single image is usually an under-constrained problem. Researchers thus change to exploit multiple images or extra knowledge to advance the dehazing technique. For instance, [6]-[8] require multiple images taken from the same scene under different kinds of weather, and some use rough depth information given by user or from known 3D models [9, 10], while [11]-[13] propose to utilize several images taken with diverse polarization degrees. However, the required supplementary information is not always available, which impedes the progress of this line of research.</p>
<p>With stronger assumptions and better priors, the dehazing algorithms depending upon single image have made remarkable progresses recently. &hellip;</p>
<p>==&gt;</p>
<p>Recently remarkable progresses have been made on single image dehazing owing to stronger assumptions and better priors. Tan proposed a Markov Random Field (MRF) based method by assuming that the local contrast of the hazy image is much lower than that of corresponding haze-free version [14]. His results are visually encouraging but tend to be over-saturated in a few local regions. Fattal assumed that the transmission is locally uncorrelated with surface shading, and thus estimated the medium transmission through the albedo of the scene [15]. Although his approach is able to produce impressive results in some cases, the assumption fails in many real-life scenarios (e.g. heavy haze). Still other methods have resorted to better haze-relevant priors, especially when removing haze from single image. Tang et al. [16] used Random Forests (RF) to fuse four kinds of haze-relevant features [17], and obtained haze-free images by estimating the transmission. Zhu et al. [18] instead used a linear model with color attenuating prior to infer the scene depth for estimating transmission. Despite significant progresses with single image haze removal, they still have difficulty in coping with complex hazy scenes, which often challenge the available assumptions and priors.</p>
<p>Despite significant progresses, some of above methods have difficulty in coping with complex hazy scenes, owing to their vulnerabilities caused by strong assumptions and priors. Compared with above methods, human eyes can easily recognize hazy region without any additional restriction. &hellip;</p>
<p>==&gt;</p>
<p>However, these pioneer work mitigated the nonlinear spatial-variant degradation to some extent because they are sensitive to their assumptions they made. In point human eyes do not need extra computational assumptions but can understand hazy region reliably and &ldquo;see&rdquo; clear image in heads without any restriction. Therefore, one appealing solution on image dehazing is the biologically inspired approach, for example convolutional neural network (CNN). The bio-inspired CNNs have impressively advanced the performance
of computer vision task such as</p>
<p>&hellip; The design of our model does not follow the assumptions mentioned above, thus the limitations of our model are fewer than that of the preceding dehazing methods. &hellip;</p>
<p>==&gt;</p>
<p>&hellip; The first extension is that we no longer require assumptions or priors, which distinguishes our approach from most existing work. The second is that we take full hazy image as input rather than cut an image into small patches as DehazeNet did in training stage. Instead our model &hellip;</p>
<p>In the proposed method, a novel deep convolutional neural network is used as the regional detection model. From the perspective of function, this model is mainly composed of two functional parts. </p>
<p>==&gt;</p>
<p>The proposed model is mainly composed of two functional parts: one is a regional-feature extraction, the other is a non-linear regression model that maps the regional features into transmission. </p>
<p>a light version of our model is proposed for mobile device</p>
<p>==&gt;</p>
<p>a light version of our model is curtailed from the base model for use on mobile devices.</p>
<p>but also two times the processing speed of the proposed model.</p>
<p>==&gt;</p>
<p>but also quicker processing speed, about two times shorter than the base model.</p>
<p>Some typos, errors and minor questions:</p>
<pre><code>p1. fog and mist ==&gt; fog, mist and haze
p1. dehazing for single image ==&gt; for删掉
p3. with an aim of building ==&gt; to
p3. perfected by? 是完善的意思就不改
p3. perfected by Narasimhan [5] and Nayar [8], on the original model initiated by McCartney [47] ==&gt;, initiated by McCartney [47] and perfected by Narasimhan [5] and Nayar [8].
p3. As shown by Algorithm 1 ==&gt; As described by Algorithm 1
p4. However, ==&gt; In reality,
p4. Here, A ==&gt; In Eq. (2), A
p4. So far ${\tilde{t}}$ has become the only unknown in Algorithm 1 and Eq. (2) ==&gt; So ${\tilde{t}}$ is the only unknown variable in Algorithm 1 and Eq. (2)
p4. introduce the RPN ==&gt; introduced
p4. design a RPN ==&gt; design a RPN variance
p5. As shown in Fig. 4, the overall structure ... ==&gt; The overall structure ... 删回车直接接上段
p5. exist ==&gt; exists
p6. implements two-staged procedure ==&gt; implements a two-staged procedure
p6. named Unit A ==&gt; Unit A 
p6. denotes ==&gt; denote
p6. who ==&gt; that
p6. [24]. ==&gt; .
p6.  “direct” 发现有时用单引号，有时用双引号
p7. proposed to run on ==&gt; evaluated on a
p7. With the aim of increasing ==&gt; To increase
p7. Thus ==&gt; Hence
P7. as $\textbf{DATA.I}$. ==&gt;, denoted as $\textbf{DATA.I}$.
P7. as $\textbf{DATA.II}$. ==&gt;, denoted as $\textbf{DATA.II}$.
p7. information entropy of the image ==&gt; the information entropy 
p7. both of these methods ==&gt; both methods
p7. these results ==&gt; their results
p7.  we can see that our method obtains higher information entropy in all results shown in Fig. 8 ==&gt;  we can read in the description of Fig. 8 that our method obtains higher information entropy in all instances
p8. It can be seen from the above table that the deep learning ==&gt; It can be
seen that the proposed
p8. advantageous ==&gt; competitive 
p8. Qualitative Evaluation ==&gt; Quantitative and Qualitative Evaluation
p9. result. ==&gt; counterpart.
p9. . Table IV summarizes the results of all approaches on ==&gt;, while Table IV summarizes their performance on
p9. A smaller value of MSE along with bigger values of SSIM, PSNR and WPSNR suggests a better restoration effect. ==&gt; Smaller MSE and bigger SSIM, PSNR and WPSNR indicate a better restoration effect.
p9. different methods ==&gt; comparative methods
p9. advantages over ==&gt; than 
p9. In addition, ==&gt; Especially
p9. which are the ==&gt; which signal the
p9. discoloured, ==&gt; discoloured
p9. In the first three results ... ==&gt; For the first three images, FVR appears to change or distort their style.
p9. Compared with above methods, deep learning based method shows ==&gt; Compared with other methods, deep learning based methods show
p9. in all results. ==&gt; in all instances.
p9. which is useful ==&gt; which is confirmed useful
p9. retain ==&gt; retains
p9. The analysis of Fig. 10 shows that ==&gt; It can be concluded from Fig. 10 that
p9. haze-free images well from ==&gt; 删
p9. different illumination ==&gt; various illumination
p9-10. D. Application for low-power devices ==&gt; D. Application for image enhancement and low-power devices
p12. Overall, ==&gt; Hence 
p13. RPN into ==&gt; a RPN variance for
p13. dehazing. And we propose ==&gt; dehazing with an emphasis on 
p13. used to ==&gt; to
p13. end-to-end ==&gt; in a end-to-end manner
p13. We can use ==&gt; We are trying
p13. we can ==&gt; we will
p1-p15. clean image(s) ==&gt; clear
p1-p15. veil
</code></pre>
<p>can help you to dramatically improve an image by removing haze. The Dehaze technology is based on a physical model of how light is transmitted, and it tries to estimate light that is lost due to absorption and scattering through the atmosphere. For the best results, you’ll want to set the white balance for the image before using Dehaze. Then, in the Effects panel, move the slider to the right – to easily remove the haze from the original scene. Move the slider to the left to add a creative haze effect.You can choose to make very subtle to very significant adjustments – if you’re pushing the slider to the extreme, you might want to refine the image using the Basic panel (increasing the shadow detail or refining the Vibrance slider) in order to achieve the exact look that you’re after. </p>
<p>The deep learning-based method has the ability to learn adaptive regression models for different weather conditions, which is able to restore the foggy image with inhomogeneous haze or dense haze.</p>
<p>DeHaze analyzes your image and creates an atmospheric profile tailored specifically to the image to mathematically  reverse the effect of haze. </p>
<p>Haze and fog act like an image filter, reducing saturation and contrast. Traditional fog reduction filters introduce artifacts. The Topaz Studio DeHaze Adjustment creates a tailored layer adjustment for each image that allows users to bring life back to images with a completely natural result.</p>
<p>Most DeHaze tools simply boost contrast and saturation to combat haze. Topaz DeHaze builds an atmospheric profile of the haze in your image, including estimation of the depth of your subject to accurately reverse the effects of atmospheric haze, smoke, fog, or light pollution in your image. DeHaze reverses the effect that atmospheric light reflection has on images. The result is natural, clear images that boast rejuvenated contrast and color saturation.</p>
<p>Thick haze is hard, if not impossible, to eliminate completely. Even though photographers can avail themselves of a certain number of tools, such as anti-haze or polarized filters, their effectiveness is limited.</p>
<div class="pageBottom"><span class="bNum">3</span><span class="bTit">THE APPROACH DESCRIPTION</span><a id="dehaze3"></a><span class="tBot"><a href="#hcomp" target="_self" > Top</a> </span></div>

<p>「tai: 第三章整体思路没有变化，但是改动了几个地方。
第一：弱化了RPN对文章的影响，改为区域检测网络。
第二：在用导向滤波计算fine transmission时，改动了语句，因为之前的语句重复率高了点。</p>
<p>Fig. 2 illustrates how haze and fog act like an image ﬁlter to reduce saturation and contrast. Typically, dehazing technology builds an atmospheric proﬁle of haze in the image, including estimate of the depth of the subject to accurately reverse the effects on atmospheric haze, smoke, fog, or light pollution in the image.</p>
<p>==&gt;</p>
<p>The particles or droplets in haze reduce the clarity of the atmosphere by the actual obscuring of what is behind them or light reflection in them, leading to low visibility and contrast as illustrated by Fig. 2. The dehazing technology should estimate light that is lost due to absorption and scattering through the atmosphere. Typically, it tries to build an atmospheric proﬁle of haze in the image, including estimate of the depth of the subject to accurately reverse the effects on atmospheric haze, smoke, fog, or light pollution in the image.</p>
<p>Our haze removal approach includes three main steps: estimating the transmission maps of all patches in an image, estimating atmospheric light via the corresponding gray-scale image and the transmission map, and recovering the scene radiance (i.e. mathematically reverse the effect on haze with the estimated transmission map). The whole process of haze removal by the proposed method is shown in Fig. 3. </p>
<p>==&gt;</p>
<p>Our haze removal approach, as illustrated by Fig.3, includes three main steps: estimating the transmission maps of all patches in an image, estimating atmospheric light via the corresponding gray-scale image and the transmission map, and recovering the scene radiance (i.e. mathematically reverse the effect on haze with the estimated transmission map). </p>
<p>Ren et al. [23] introduce the Region Proposal Network (RPN) within Faster R-CNN architecture with an aim to providing region proposal for object detection. In the original design, the RPN consists of two parts: the first part is shared convolution layer, and each pixel on the last layer of this part corresponds to   anchors with different scales at the corresponding position in the input image. The first part shares its convolutional layer with Fast R-CNN [48] which is used for object detection. The second part is a small sub-network, consisting mainly of a   sliding window. The sliding window mainly acts on the last layer of shared convolution layers, and extracts the feature vector of each proposal. Then this feature is fed into two sibling fully connected layers &ndash; a box-regression layer and a box-classification layer.</p>
<p>==&gt;</p>
<p>Ren et al. [23] introduce the Region Proposal Network (RPN) within Faster R-CNN architecture with an aim to providing region proposal for object detection. In the original design, the RPN consists of two parts: one is a shared convolution layer, and the other is a small sub-network consisting mainly of a sliding window. The first part shares its convolutional layer with Fast R-CNN [48] which is used for object detection, and each pixel on the last layer of this part corresponds to anchors with different scales at the corresponding position in the input image, whereas the sliding window of the second part mainly acts on the last layer of shared convolution layers, and extracts the feature vector of each proposal. Then this feature is fed into two sibling fully connected layers &ndash; a box-regression layer and a box-classification layer.</p>
<p>Intuitively, the $1 \times 1$ convolution layer may better capture the &lsquo;gist&rsquo; of haze while the $3 \times 3$ convolution layer may better capture the details.</p>
<p>==&gt;</p>
<p>Intuitively, the $1 \times 1$ convolution layer may capture the &lsquo;gist&rsquo; of haze better while the $3 \times 3$ convolution layer may capture the details better.</p>
<div class="pageBottom"><span class="bNum">4</span><span class="bTit">IV. EXPERIMENTS</span><a id="dehaze4"></a><span class="tBot"><a href="#hcomp" target="_self" > Top</a> </span></div>

<p>「tai: 第四章：
主要加了两个实验
第一：在拆分实验那里，DCP在处理白色无阴影的object时会出现问题，我们用图8c展示了一下，并且展示出我们的方法不会有这个问题
第二：增加了水下场景增强实验。
第三：按照李老师的建议，补充移动端模型的介绍。」
—————————</p>
<p>Since it is hard to collect a large number of clean/hazy image pairs from natural scenes, we respectively synthesize hazy images for 1000 clear images collected from the Internet to train the network. With the aim of increasing the amount of data used in training, we randomly choose twenty $224 \times 224$ patches from each image and synthesize hazy patches using Eq. (1). Here, the $t$ is a random transmission from range [0.1, 1]; the $A$ is a random atmospheric light from range [0.7, 1.1]. Thus, a total of 20 thousand pairs of 224$\times$224 clean/hazy patches are generated to train the network.</p>
<p>==&gt;</p>
<p>Despite easy to obtain images and pictures nowdays, it is still hard to collect a large number of hazy images with their matching (or pairing) clear ones. So we synthesize hazy images based on 1000 clear images collected from the Internet in order to train the network. To increase the amount of data for training, we randomly choose twenty $224 \times 224$ patches from each image and synthesize hazy patches using Eq. (1) such that the random transmission $t$ is within [0.1, 1] and the random atmospheric light $A$ $\in$ [0.7, 1.1]. Hence, a total of 20 thousand pairs of 224$\times$224 clean/hazy patches are generated to train the network.</p>
<p>It seems that these results are very similar, but compared with DCP, we can see that our method obtains higher information entropy in all results shown in Fig. 8.  &hellip;</p>
<p>==&gt;</p>
<p>It is obvious that they both improve the visibility of the hazy images, and DCP seemingly yields images with high color contrast, but compared quantitatively, we can read in the description of Fig. 8 that our method obtains higher information entropy in all instances. Owing to the low contrast of the hazy images, the pixels in them are so close that the information entropy is low if processed by DCP. Instead our model makes good use of CNN to estimate the transmission map more precisely, as reported in Table II. As shown in Fig. 8 (c) also, DCP handles clouds stubbornly compared with our method. This is because DCP is based on statistic, but it may underestimate the transmission for some scene objects if no shadow is cast on them for their brightness is inherently similar to the atmospheric light.</p>
<p>Besides DCP, the first model we compare is ZFNet [55], which is commonly used for building a RPN model in object detection [56]; the second model is ResNet, which is only composed of standard ResNet unit (cascaded $3 \times 3$-convolution with identity map). ZFNet and ResNet are both trained on the same training data.</p>
<p>==&gt;</p>
<p>The first comparative model is DCP, the second is ZFNet [55] which is commonly used for building a RPN model in object detection [56]. The third is ResNet which is only composed of standard ResNet unit (cascaded $3 \times 3$-convolution with identity map). ZFNet and ResNet are both trained on the same training data.</p>
<p>In this study, we curtail the proposed model to meet the &hellip;</p>
<p>==&gt;</p>
<p>We also conduct two more experiments to verify the possibility to scale up to real scenarios. </p>
<p>D1. Image enhancement</p>
<p>In the first study, we try to apply the proposed model to enhance similar images degraded due to underwater, mist and halo. Unlike haze, there are other bad factors such as dim light, halation and the humidity that blur the image content. As shown by Fig. 12, our method can get satisfactory results in mitigating mist and halo. Besides, the proposed method was applied to visibility enhancement for underwater scene, a challenge task often causing dark channel invalid. As shown in Fig. 13, our method also achieves good results in this task.</p>
<p>Moreover, Table V gives the average time consumption of estimating the transmissions of hazy images with special size. It is worth noting that the program exectuion time in the Table V is based on our mobile device.</p>
<p>==&gt;</p>
<p>Moreover, Table V compares the average time consumption when estimating the transmissions of hazy images by using the original model and by using the curtailed one. It can be conclude that the program execution time shorter on our mobile device while dehazing results in Fig. 14 are nice.</p>
<h2 id="work-history">Work History <a id="his"></a><a class="headerlink" href="#work-history" title="Permanent link"></a></h2>
<ul>
<li>2018-02-18 (周日) 20:22 新建md文件</li>
<li>2018-02-20 (周二) 21:00 文章修改准备</li>
<li>2018-03-11 (周日) 09:52 fyl文章修改</li>
<li>2018-03-11 (周日) 13:14 yx文章修改准备</li>
<li>2018-03-14 (周三) 10:59 yx文章修改</li>
<li>2018-04-15 (周日) 08:34 yx文章修改</li>
<li>2020-11-17 (周二) 08:24 更新md文件</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>[Zhang et. al., 2014] Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang. Facial Landmark Detection by Deep Multi-task Learning, in Proceedings of European Conference on Computer Vision (ECCV), 2014&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>[Yin and Liu, 2017] Xi Yin, Xiaoming Liu. Multi-Task Convolutional Neural Network for Pose-Invariant Face Recognition. IEEE Transactions on Image Processing, sAug. 2017 (in press)&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>[Ranjan et. al., 2016] Rajeev Ranjan, Vishal M. Patel, Rama Chellappa. HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence. PP(99), March 2016. DOI: 10.1109/TPAMI.2017.2781233&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div></article></body></html>